{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *\n",
    "from data import *\n",
    "from utils import *\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/data_sample.csv\", sep=\"|\")\n",
    "data = df[\"headline\"].str.strip() + \" \" + df[\"text\"].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_config = {\n",
    "    'type': 'gru',\n",
    "    'params': {\n",
    "        'hidden_size':128,\n",
    "        'num_layers': 1\n",
    "    }\n",
    "}\n",
    "\n",
    "nn_config = {\n",
    "    'in_features': 128,\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Vocab from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader, NUM_CLASSES, dataset = get_dataloaders(\n",
    "    file=\"data/data_sample.csv\",\n",
    "    tokenizer=Tokenizer()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(dataset.vocab)\n",
    "model = RNN.from_data(rnn_config=rnn_config, \n",
    "                      nn_config=nn_config, \n",
    "                      NUM_CLASSES=NUM_CLASSES,\n",
    "                      vocab_size=VOCAB_SIZE,\n",
    "                      embedding_dim=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=model.parameters())\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\tloss: 0.679\n",
      "epoch: 1\tloss: 0.662\n",
      "epoch: 2\tloss: 0.645\n",
      "epoch: 3\tloss: 0.629\n",
      "epoch: 4\tloss: 0.612\n",
      "epoch: 5\tloss: 0.593\n",
      "epoch: 6\tloss: 0.573\n",
      "epoch: 7\tloss: 0.552\n",
      "epoch: 8\tloss: 0.528\n",
      "epoch: 9\tloss: 0.502\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "EPOCH = 10\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    loss = train(optimizer=optimizer, criterion=criterion, model=model, train_loader=train_loader)\n",
    "    print(f\"epoch: {epoch}\\tloss: {loss:.3f}\")\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 0.4867036044597626, 'acc': tensor(39.6667)}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model=model, criterion=criterion, test_loader=test_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Vocab from GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available pretrained:\n",
      "charngram.100d functools.partial(<class 'torchtext.vocab.vectors.CharNGram'>)\n",
      "fasttext.en.300d functools.partial(<class 'torchtext.vocab.vectors.FastText'>, language='en')\n",
      "fasttext.simple.300d functools.partial(<class 'torchtext.vocab.vectors.FastText'>, language='simple')\n",
      "glove.42B.300d functools.partial(<class 'torchtext.vocab.vectors.GloVe'>, name='42B', dim='300')\n",
      "glove.840B.300d functools.partial(<class 'torchtext.vocab.vectors.GloVe'>, name='840B', dim='300')\n",
      "glove.twitter.27B.25d functools.partial(<class 'torchtext.vocab.vectors.GloVe'>, name='twitter.27B', dim='25')\n",
      "glove.twitter.27B.50d functools.partial(<class 'torchtext.vocab.vectors.GloVe'>, name='twitter.27B', dim='50')\n",
      "glove.twitter.27B.100d functools.partial(<class 'torchtext.vocab.vectors.GloVe'>, name='twitter.27B', dim='100')\n",
      "glove.twitter.27B.200d functools.partial(<class 'torchtext.vocab.vectors.GloVe'>, name='twitter.27B', dim='200')\n",
      "glove.6B.50d functools.partial(<class 'torchtext.vocab.vectors.GloVe'>, name='6B', dim='50')\n",
      "glove.6B.100d functools.partial(<class 'torchtext.vocab.vectors.GloVe'>, name='6B', dim='100')\n",
      "glove.6B.200d functools.partial(<class 'torchtext.vocab.vectors.GloVe'>, name='6B', dim='200')\n",
      "glove.6B.300d functools.partial(<class 'torchtext.vocab.vectors.GloVe'>, name='6B', dim='300')\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "\n",
    "print(\"Available pretrained:\")\n",
    "for k, v in torchtext.vocab.pretrained_aliases.items():\n",
    "    print(k,v)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 GloVe 50d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader, NUM_CLASSES, dataset = get_dataloaders(\n",
    "    file=\"data/data_sample.csv\",\n",
    "    tokenizer=Tokenizer(),\n",
    "    vocab_from=\"glove.6B.50d\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(dataset.vocab)\n",
    "model = RNN.from_glove(rnn_config=rnn_config,\n",
    "                       nn_config=nn_config,\n",
    "                       NUM_CLASSES=dataset.NUM_CLASSES,\n",
    "                       glove_vectors=dataset.vectors,\n",
    "                       embedding_dim=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=model.parameters())\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\tloss: 0.682\n",
      "epoch: 1\tloss: 0.662\n",
      "epoch: 2\tloss: 0.643\n",
      "epoch: 3\tloss: 0.623\n",
      "epoch: 4\tloss: 0.605\n",
      "epoch: 5\tloss: 0.585\n",
      "epoch: 6\tloss: 0.566\n",
      "epoch: 7\tloss: 0.545\n",
      "epoch: 8\tloss: 0.524\n",
      "epoch: 9\tloss: 0.503\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "EPOCH = 10\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    loss = train(optimizer=optimizer, criterion=criterion, model=model, train_loader=train_loader)\n",
    "    print(f\"epoch: {epoch}\\tloss: {loss:.3f}\")\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 0.4879262149333954, 'acc': tensor(40.3333)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model=model, criterion=criterion, test_loader=test_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 GloVe 200d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader, NUM_CLASSES, dataset = get_dataloaders(\n",
    "    file=\"data/data_sample.csv\",\n",
    "    tokenizer=Tokenizer(),\n",
    "    vocab_from=\"glove.6B.200d\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(dataset.vocab)\n",
    "model = RNN.from_glove(rnn_config=rnn_config,\n",
    "                       nn_config=nn_config,\n",
    "                       NUM_CLASSES=dataset.NUM_CLASSES,\n",
    "                       glove_vectors=dataset.vectors,\n",
    "                       embedding_dim=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=model.parameters())\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\tloss: 0.719\n",
      "epoch: 1\tloss: 0.694\n",
      "epoch: 2\tloss: 0.669\n",
      "epoch: 3\tloss: 0.648\n",
      "epoch: 4\tloss: 0.625\n",
      "epoch: 5\tloss: 0.604\n",
      "epoch: 6\tloss: 0.582\n",
      "epoch: 7\tloss: 0.561\n",
      "epoch: 8\tloss: 0.539\n",
      "epoch: 9\tloss: 0.518\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "EPOCH = 10\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    loss = train(optimizer=optimizer, criterion=criterion, model=model, train_loader=train_loader)\n",
    "    print(f\"epoch: {epoch}\\tloss: {loss:.3f}\")\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 0.48572081327438354, 'acc': tensor(39.3333)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model=model, criterion=criterion, test_loader=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
